{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f983fa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda_hackaboss\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda_hackaboss\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: sqlalchemy in d:\\anaconda_hackaboss\\lib\\site-packages (2.0.34)\n",
      "Requirement already satisfied: psycopg2-binary in d:\\anaconda_hackaboss\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda_hackaboss\\lib\\site-packages (1.5.1)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: joblib in d:\\anaconda_hackaboss\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda_hackaboss\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda_hackaboss\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda_hackaboss\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in d:\\anaconda_hackaboss\\lib\\site-packages (from sqlalchemy) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda_hackaboss\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda_hackaboss\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda_hackaboss\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda_hackaboss\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.5 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy sqlalchemy psycopg2-binary scikit-learn lightgbm joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c488e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import timedelta, datetime, date\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# =========\n",
    "# Config\n",
    "# =========\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"postgres\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"postgres\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")  # ponlo vía Secrets o env var\n",
    "TABLE_NAME = os.getenv(\"TABLE_NAME\", \"demanda_ree\")\n",
    "\n",
    "MODEL_PATH = \"/tmp/lgb_demand_model.joblib\"   # /tmp en Lambda\n",
    "FEATURES_PATH = \"/tmp/features_cols.txt\"\n",
    "\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET\")\n",
    "S3_MODEL_KEY = os.getenv(\"S3_MODEL_KEY\", \"models/demanda/lgbm/model.joblib\")\n",
    "S3_FEATURES_KEY = os.getenv(\"S3_FEATURES_KEY\", \"models/demanda/lgbm/features_cols.txt\")\n",
    "\n",
    "API_URL_RELOAD = os.getenv(\"API_URL_RELOAD\", \"\")\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\", \"\")\n",
    "\n",
    "# Usa psycopg2-binary (simple) o cambia a psycopg v3 si prefieres\n",
    "CONN_STR = f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(CONN_STR, pool_pre_ping=True)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1) EXTRAER DATOS DE AURORA\n",
    "# ============================\n",
    "def load_data_from_rds(limit_rows=None):\n",
    "    query = f'SELECT id, fecha, demanda_mw FROM {TABLE_NAME} ORDER BY fecha ASC'\n",
    "    if limit_rows:\n",
    "        query += f\" LIMIT {int(limit_rows)}\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741763e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lee Aurora,\n",
    "\n",
    "entrena LightGBM,\n",
    "\n",
    "sube model.joblib + features_cols.txt a S3,\n",
    "\n",
    "genera 7 días y los guarda en Aurora,\n",
    "\n",
    "(opcional) llama a /reload-model de tu API.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import timedelta, datetime, date\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# =========\n",
    "# Config\n",
    "# =========\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"postgres\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"postgres\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")  # ponlo vía Secrets o env var\n",
    "TABLE_NAME = os.getenv(\"TABLE_NAME\", \"demanda_ree\")\n",
    "\n",
    "MODEL_PATH = \"/tmp/lgb_demand_model.joblib\"   # /tmp en Lambda\n",
    "FEATURES_PATH = \"/tmp/features_cols.txt\"\n",
    "\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET\")\n",
    "S3_MODEL_KEY = os.getenv(\"S3_MODEL_KEY\", \"models/demanda/lgbm/model.joblib\")\n",
    "S3_FEATURES_KEY = os.getenv(\"S3_FEATURES_KEY\", \"models/demanda/lgbm/features_cols.txt\")\n",
    "\n",
    "API_URL_RELOAD = os.getenv(\"API_URL_RELOAD\", \"\")\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\", \"\")\n",
    "\n",
    "# Usa psycopg2-binary (simple) o cambia a psycopg v3 si prefieres\n",
    "CONN_STR = f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(CONN_STR, pool_pre_ping=True)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1) EXTRAER DATOS DE AURORA\n",
    "# ============================\n",
    "def load_data_from_rds(limit_rows=None):\n",
    "    query = f'SELECT id, fecha, demanda_mw FROM {TABLE_NAME} ORDER BY fecha ASC'\n",
    "    if limit_rows:\n",
    "        query += f\" LIMIT {int(limit_rows)}\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2) PREPROCESS / FEATURE ENGINEERING\n",
    "# =========================================\n",
    "def preprocess_and_features(df):\n",
    "    df = df.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha']).dt.date\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    df = df.sort_values('fecha').drop_duplicates(subset='fecha').reset_index(drop=True)\n",
    "\n",
    "    # Features temporales\n",
    "    df['dayofweek'] = df['fecha'].dt.dayofweek\n",
    "    df['month'] = df['fecha'].dt.month\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
    "\n",
    "    # Lags\n",
    "    for l in [1, 7, 14, 30]:\n",
    "        df[f'lag_{l}'] = df['demanda_mw'].shift(l)\n",
    "\n",
    "    # Rolling\n",
    "    for w in [7, 14, 30]:\n",
    "        df[f'roll_mean_{w}'] = df['demanda_mw'].shift(1).rolling(window=w, min_periods=1).mean()\n",
    "        df[f'roll_std_{w}'] = df['demanda_mw'].shift(1).rolling(window=w, min_periods=1).std().fillna(0)\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14eb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================\n",
    "# 3) TRAIN/SPLIT temporal\n",
    "# ====================================\n",
    "def temporal_train_test_split(df, val_size_days=500, test_size_days=500):\n",
    "    n = len(df)\n",
    "    if n < (val_size_days + test_size_days + 10):\n",
    "        raise ValueError(\"Muy pocos datos para dividir en train/val/test.\")\n",
    "    train_end = n - val_size_days - test_size_days\n",
    "    val_end = n - test_size_days\n",
    "    train = df.iloc[:train_end].copy()\n",
    "    val = df.iloc[train_end:val_end].copy()\n",
    "    test = df.iloc[val_end:].copy()\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695a638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 4) ENTRENAR LGBM\n",
    "# ======================\n",
    "def train_lightgbm(train_df, val_df, features, target='demanda_mw', params=None):\n",
    "    train_data = lgb.Dataset(train_df[features], label=train_df[target])\n",
    "    val_data = lgb.Dataset(val_df[features], label=val_df[target])\n",
    "    default_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"num_leaves\": 31,\n",
    "        \"n_estimators\": 1000,\n",
    "    }\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "    gbm = lgb.train(\n",
    "        default_params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "    return gbm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fbe77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 5) EVALUAR\n",
    "# ======================\n",
    "def evaluate_model(model, df, features, target='demanda_mw'):\n",
    "    preds = model.predict(df[features])\n",
    "    mae = mean_absolute_error(df[target], preds)\n",
    "    rmse = mean_squared_error(df[target], preds, squared=False)\n",
    "    mape = np.mean(np.abs((df[target] - preds) / (df[target] + 1e-9))) * 100\n",
    "    accuracy = 100 - mape\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE(%)\": mape, \"ACC(%)\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0977340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# 6) PREDECIR 7 DÍAS\n",
    "# =====================================================\n",
    "def predict_future(model, last_df, features, horizon=7):\n",
    "    df_work = last_df.copy().sort_values('fecha').reset_index(drop=True)\n",
    "    preds = []\n",
    "    for _ in range(horizon):\n",
    "        next_time = df_work['fecha'].iloc[-1] + pd.Timedelta(days=1)\n",
    "        row = {\"fecha\": next_time}\n",
    "        row['dayofweek'] = next_time.dayofweek\n",
    "        row['month'] = next_time.month\n",
    "        row['is_weekend'] = int(next_time.dayofweek in [5,6])\n",
    "        for l in [1,7,14,30]:\n",
    "            row[f'lag_{l}'] = df_work['demanda_mw'].iloc[-l] if l <= len(df_work) else np.nan\n",
    "        for w in [7,14,30]:\n",
    "            vals = df_work['demanda_mw'].shift(1).iloc[-w:] if len(df_work) >= 1 else []\n",
    "            if len(vals) > 0:\n",
    "                row[f'roll_mean_{w}'] = float(vals.mean())\n",
    "                row[f'roll_std_{w}'] = float(vals.std()) if len(vals) > 1 else 0.0\n",
    "            else:\n",
    "                row[f'roll_mean_{w}'] = np.nan\n",
    "                row[f'roll_std_{w}'] = np.nan\n",
    "        row_df = pd.DataFrame([row])\n",
    "        for c in features:\n",
    "            if c not in row_df.columns:\n",
    "                row_df[c] = np.nan\n",
    "        row_df = row_df[features].fillna(df_work[features].mean(numeric_only=True))\n",
    "        pred = float(model.predict(row_df)[0])\n",
    "        preds.append({\"fecha\": next_time, \"demanda_mw\": pred})\n",
    "        df_work = pd.concat([df_work, pd.DataFrame([{\"fecha\": next_time, \"demanda_mw\": pred}])], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71410d04-2b79-4779-a6d3-ecd03afabcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 7) GUARDAR FORECAST EN AURORA (UPSERT)\n",
    "# =======================================================\n",
    "def save_forecast_to_db(preds_df, model_version=\"v1\", ambito=\"ES\"):\n",
    "    if preds_df.empty:\n",
    "        logger.warning(\"No hay predicciones para guardar.\")\n",
    "        return\n",
    "    fdate = date.today()\n",
    "    rows = [\n",
    "        {\n",
    "            \"forecast_date\": fdate,\n",
    "            \"target_date\": pd.to_datetime(r[\"fecha\"]).date(),\n",
    "            \"ambito\": ambito,\n",
    "            \"y_hat\": float(r[\"demanda_mw\"]),\n",
    "            \"model_version\": model_version\n",
    "        }\n",
    "        for _, r in preds_df.iterrows()\n",
    "    ]\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO demanda_ree_forecast (forecast_date, target_date, ambito, y_hat, model_version)\n",
    "        VALUES (:forecast_date, :target_date, :ambito, :y_hat, :model_version)\n",
    "        ON CONFLICT (forecast_date, target_date, ambito)\n",
    "        DO UPDATE SET y_hat = EXCLUDED.y_hat, model_version = EXCLUDED.model_version\n",
    "    \"\"\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(sql, rows)\n",
    "    logger.info(f\"Guardadas {len(rows)} predicciones en demanda_ree_forecast (forecast_date={fdate}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ed1ee-72e0-4a76-ae76-5a036d406758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8) SUBIR ARTEFACTOS A S3\n",
    "# ==========================================\n",
    "def upload_artifacts_to_s3(model_path, features_path):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        s3.upload_fileobj(f, S3_BUCKET, S3_MODEL_KEY)\n",
    "    with open(features_path, \"rb\") as f:\n",
    "        s3.upload_fileobj(f, S3_BUCKET, S3_FEATURES_KEY)\n",
    "    logger.info(f\"Subidos a s3://{S3_BUCKET}/{S3_MODEL_KEY} y s3://{S3_BUCKET}/{S3_FEATURES_KEY}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6d122-10a7-46fe-903e-abb0584d891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 9) (Opcional) NOTIFICAR A LA API /reload-model\n",
    "# ===================================================\n",
    "def notify_api_reload():\n",
    "    if not API_URL_RELOAD:\n",
    "        logger.info(\"API_URL_RELOAD vacío: no se notifica a la API.\")\n",
    "        return\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if API_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "    try:\n",
    "        r = requests.post(API_URL_RELOAD, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        logger.info(\"API /reload-model notificada OK: %s\", r.text)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"No se pudo notificar a /reload-model: %s\", e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf1f71-dd69-4ec9-869e-97635236e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 10) HANDLER LAMBDA\n",
    "# =========================\n",
    "def lambda_handler(event, context):\n",
    "    logger.info(\"Inicio job entrenamiento+forecast\")\n",
    "    df_raw = load_data_from_rds()\n",
    "    logger.info(\"Registros extraídos: %s\", len(df_raw))\n",
    "\n",
    "    df_proc = preprocess_and_features(df_raw)\n",
    "    logger.info(\"Tras preprocess: %s\", len(df_proc))\n",
    "\n",
    "    train, val, test = temporal_train_test_split(df_proc)\n",
    "    excluded = ['id', 'fecha', 'demanda_mw']\n",
    "    features = [c for c in df_proc.columns if c not in excluded]\n",
    "    with open(FEATURES_PATH, 'w') as f:\n",
    "        f.write(\"\\n\".join(features))\n",
    "    logger.info(\"Num features: %s\", len(features))\n",
    "\n",
    "    model = train_lightgbm(train, val, features)\n",
    "    joblib.dump(model, MODEL_PATH)\n",
    "    logger.info(\"Modelo guardado en /tmp\")\n",
    "\n",
    "    # métricas (logs)\n",
    "    logger.info(\"Eval TRAIN: %s\", evaluate_model(model, train, features))\n",
    "    logger.info(\"Eval VAL  : %s\", evaluate_model(model, val, features))\n",
    "    logger.info(\"Eval TEST : %s\", evaluate_model(model, test, features))\n",
    "\n",
    "    upload_artifacts_to_s3(MODEL_PATH, FEATURES_PATH)\n",
    "\n",
    "    # Predicción 7 días y guardado\n",
    "    last_block = df_proc.iloc[-180:].copy()\n",
    "    preds_future = predict_future(model, last_block, features, horizon=7)\n",
    "    logger.info(\"Predicciones generadas: %s\", len(preds_future))\n",
    "    model_version = datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "    save_forecast_to_db(preds_future, model_version=model_version, ambito=\"ES\")\n",
    "\n",
    "    # Avisar a API\n",
    "    notify_api_reload()\n",
    "\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps({\"msg\": \"OK\", \"preds\": len(preds_future)})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ea094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
